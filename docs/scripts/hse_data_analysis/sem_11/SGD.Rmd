---
title: "Ликбез по производной, градиент и метод градиентного спуска"
author: "Зарманбетов Ахмед"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 4
    highlight: pygments
    #df_print: paged
editor_options: 
  chunk_output_type: console
---
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(ggplot2)
library(tibble)
library(dplyr)
```

### Функция и ее производная

Вспомним, что **функция** -- это отображение одного множества в другое, но так, что каждому элементу одного множества соответствует один и только один элемент из другого множества. Записывается это так: $f: \mathbb{R}^n \rightarrow \mathbb{R}$. 

Вспомним простую функцию $y = x^2$. Давайте нарисуем её!

```{r}
square <- function(x){
  return(x^2)
}

df <- tibble(x = seq(-30, 30, by = 0.1),
             y = square(x))

ggplot(df, aes(x, y)) +
  geom_line()
```

В данном случае $y$ -- это функция от одной переменной $x$. На таком графике мы можем явно увидеть зависимость функции от ее аргумента. Видим, что минимум находится в точке $x = 0$. Чуть позже мы будем работать с функциями нескольких переменных.

Теперь вспомним что такое производная. Не будем лезть в тяжелую математику. Нам будет важно следующее определение. **Производная** -- скорость изменения функции в данной точке.

Вспомним некоторые производные:

1. $c' = 0, c = const$
2. $(x^n)'  = nx^{n-1}$
3. $(e^x)' = e^x$
4. $(\ln{x})' = \frac{1}{x}$ 
5. $(\sin{x})' = \cos{x}$
6. $(\cos{x})' = -\sin{x}$

А также некоторые свойства:

1. $(cf(x))' = cf'(x)$ -- константу можно вынести из под знака производной
2. $(f(x) + g(x))' = f'(x) + g'(x)$ -- производная суммы это сумма производных
3. $[f(g(x))]' = f'(g(x))g'(x)$ -- производная сложной функции

А теперь порешаем некоторые примеры, чтобы лучше с этим разобраться.

1. $y = 100$
2. $y = x^2$
3. $y = 5x$
4. $y = 10x^3$
5. $y = 4x + cos(x)$
6. $y = \ln(x^2)$
7. $y = (10 - 5x)^2$

Давайте возьмем функцию $y = x^2$. Представим, что мы находимся в точке $x_0 = 5$. Значение функции в этой точке равно 25.

```{r}
x0 <- 5
ggplot(df, aes(x, y)) +
  geom_line() + 
  geom_point(x = x0, y = square(x0), color = 'blue', size = 2)
```

Производная этой функции: $y' = 2x$. . Давайте найдем ее производную в этой точке.

```{r}
der_square <- function(x){
  return(2*x)
}

der_square(x0)
```

Производная в этой точке равна 10. Важное свойство производной состоит в том, что если производная положительна, то чтобы увеличить значение функции надо увеличивать значение аргумента. Аналогично, если мы хотим уменьшить значение функции, мы должны уменьшить аргумент. Если значение производной в точке отрицательно, то все ровно наоборот. 

Объединяя эти два случая получаем:

$x_0 = x_0 + sign(f'(x_0)) t$

Второй вопрос, а на сколько нужно увлечивать или уменьшать?

### Минимум функции от одного аргумента

В школе нас учили находить минимум или максимум функции. Нужно было найти первую производную, приравнять ее к нулю и дело почти в шляпе. Оставалось только решить уравнение и проверить точки на минимум и максимум или найти вторую производную и понять по ней. Мы отойдем от этого подхода и будем искать минимум немного по-другому. Позже мы поймем почему мы отошли от стандартного метода.

### Метод градиентного спуска (GD)

Изучим метод градиентного спуска. Его смысл заключается в том, чтобы итеративно искать минимум функции. Давайте вернемся к функции $y = x^2$. Изначально мы не знаем в какой точке находится минимум этой функции. Давайте выберем рандомно какое-то число. Пусть это будет $x_0 = 10$. Если мы найдем производную функции в этой точке, то поймем в какую сторону нам нужно двигаться, чтобы увеличить значение функции. Мы пойдем в обратную сторону, так как мы хотим минимизировать нашу функцию. Поэтому изменим значение нашего аргумента $x_0$ на величину производной. Только эта величина может принимать достаточно большие значения, поэтому давайте умножим ее на константу $\alpha$, которую мы будем задавать сами. Эта константа называется скорость обучения.

$$x_0 = x_0 - \alpha \frac{df(x)}{dx} \bigg|_{x=x_0}$$

После того как мы сделали одну итерацию мы проделали шаг градиентного спуска. Будем продолжать его делать, пока функция не начнет уменьшаться на очень маленькую величину. Давайте попробуем проделать это в коде и визуализировать. 

```{r eval=FALSE}
alpha <- 0.1
x0 <- 10

for(i in 1:20){
  p <- ggplot(df, aes(x, y)) +
        geom_line() + 
        geom_point(x = x0, y = square(x0), color = 'blue', size = 2)
  print(p)
  x0 <- x0 - alpha * der_square(x0)
  readline('Enter:')
}
```

Скорость обучения это важная штука. Давайте попробуем ее уменьшить.

```{r eval=FALSE}
alpha <- 0.01
x0 <- 10

for(i in 1:20){
  p <- ggplot(df, aes(x, y)) +
        geom_line() + 
        geom_point(x = x0, y = square(x0), color = 'blue', size = 2)
  print(p)
  x0 <- x0 - alpha * der_square(x0)
  readline('Enter:')
}
```

Видно, что сходимость стала заметно медленее. А теперь давайте попробуем увеличить скорость обучения.

```{r eval=FALSE}
alpha <- 1.2
x0 <- 10

for(i in 1:5){
  p <- ggplot(df, aes(x, y)) +
        geom_line() + 
        geom_point(aes(x = x0, y = square(x0)), color = 'blue', size = 2)
  print(p)
  x0 <- x0 - alpha * der_square(x0)
  readline('Enter:')
}
```

Случилось ужасное. Мы перескакиваем наш минимум и алгорим начинает расходится. Со скоростью обучения надо быть аккуратнее.

Давайте возьмем функцию поинтересней.

```{r}
our_function <- function(x){
  y <- 10 * cos(x) + 0.5*x^2 -5*x
  return(y)
}

df <- tibble(x = seq(-10,10, by = 0.01),
             y = our_function(x))

ggplot(df, aes(x, y)) +
  geom_line()
```

Вот такая интересная функция. Найдем ее производную.

```{r}
der_our_function <- function(x){
  y <- -10*sin(x) + x - 5
  return(y)
}
```

Попробуем найти минимум с помощью GD.

```{r eval=FALSE}
x0 <- -10
alpha <- 0.01
for(i in 1:100){
  p <- ggplot(df, aes(x, y)) +
    geom_line() +
    geom_point(x = x0 , y = our_function(x0), color='red')
  # print(p)
  x0 <- x0 - alpha * der_our_function(x0)
  # readline('Enter:')
}
print(p)
```

Мы попали в локальный минимум и никак не можем из него выбраться. Можно попробовать настроить скорость обучения и попасть в минимум. Но в реальной задаче мы не будем видеть минимум так хорошо на графике. Здесь есть несколько решений. Нужно пытаться запускать GD из нескольких рандомных точек или использовать более мощные алгоритмы градиентного спуска. 

Например, Momentum GD. Momentum это метод, который помогает стохастическому градиентному спуску сохранять направление движения. Это осуществляется за счёт добавления в выражение дополнительного слагаемого: накопленного за предыдущие шаги градиента с весом $\alpha$.

$$ \nu_t = \alpha \nu_{t-1} + \eta \frac{df(x)}{dx} \bigg|_{x=x_0}$$
$$ x_t = x_{t-1} - \nu_t$$

```{r}
x0 <- -10
eta <-  0.05
alpha <-  0.7
nu <- 0

for(i in 1:30){
  p <- ggplot(df, aes(x, y)) +
    geom_line() +
    geom_point(x = x0 , y = our_function(x0), color='red')
  # print(p)
  nu <-  alpha * nu + eta * der_our_function(x0)
  x0 <-  x0 - nu
  # readline('Enter:')
}
print(p)
```














```{r eval=FALSE, echo=FALSE}
# для создания гифок
library(datasauRus)
library(ggplot2)
library(gganimate)

ggplot(datasaurus_dozen, aes(x=x, y=y))+
  geom_point()+
  theme_minimal() +
  transition_states(dataset, 3, 10) + 
  ease_aes('cubic-in-out')

datasaurus_dozen

library(plotly)
library(gapminder)
p <- gapminder %>%
  plot_ly(
    x = ~gdpPercap, 
    y = ~lifeExp, 
    size = ~pop, 
    color = ~continent, 
    frame = ~year, 
    text = ~country, 
    hoverinfo = "text",
    type = 'scatter',
    mode = 'markers'
  ) %>%
  layout(
    xaxis = list(
      type = "log"
    )
  )
p
gapminder

```

