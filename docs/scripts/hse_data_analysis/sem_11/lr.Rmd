---
title: "Линейная регрессия"
author: "Зарманбетов Ахмед"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    toc_depth: 4
    highlight: pygments
    #df_print: paged
editor_options: 
  chunk_output_type: console
---
<style>
h1,
h2,
h3,
h4,
h5,
h6  {
  color: #317eac;
}
</style>
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)
```

### Линейная регрессия

Одной из задач машинного обучения является задача регрессии. В задачи регрессии мы хотим предсказать переменную $Y$, которая может принимать вещественные значения $Y \in \mathbb{R}$. 

Примеры задачи регрессии:

* Сколько денег заработает магазин в следующем месяце?
* Какая завтра будет температура?
* Сколько человеку лет на фото?

Переменную $Y$ мы будем называть **таргетом** или **целевой переменной**. Для предсказания этой переменной мы будем использовать другие переменные ($X_i$), которые как-то влияют на зависимую переменную и могут помочь спрогнозировать её. Для задачи прогнозирования температуры хорошей переменной будет значение температуры за предыдущий день. Такие переменные мы будем называть **признаками**. 

![](pictures/extrapolating.png)

Представим, что нам нужно предсказать заработную плату человека ($Y$). Хорошим признаком может быть сколько лет человек получал образование ($X_1$. В данном примере у нас есть всего один признак. 

Чтобы обучить любую модель машинного обучения, нам нужны данные. Для данной задачи нам нужны данные о людях: сколько они учились и сколько зарабатывают. Эти данные будем называть **обучающей выборкой**. 

Давайте смоделируем данные для 50 людей. И посмотрим на график рассеяния, где по оси $x$ отложим количество лет обучения, а по оси $y$ доход.

```{r}
set.seed(42)
n <- 50
df <- tibble(education = seq(7, 20, length.out = n) + rnorm(n, sd = 2),
             income = 10000 + 5000 * education + rnorm(n, sd = 5000))

ggplot(df, aes(x = education, y = income)) +
  geom_point()
```

Видно, что есть линейная зависимость. То есть нашу переменную $Y$ можно линейно выразить через переменную $X$.

$$income = w_0 + w_1 education$$
 Возникает вопрос, какие значения дожны принимать $w_0$ и $w_1$? То есть как должна проходить линия через эти точки?
 
```{r}
ggplot(df, aes(x = education, y = income)) +
  geom_point() + 
  geom_abline(intercept = 25000, slope = 5000, color = 'red') + 
  geom_abline(intercept = 10000, slope = 5000, color = 'blue') + 
  geom_abline(intercept = 9000, slope = 5500, color = 'green') + 
  geom_abline(intercept = 11000, slope = 4700, color = 'orange') 
```

На графике можно увидеть разные модели, какая из них самая лучшая? Для того, чтобы понять какую модель выбрать, нам нужно ввести критерий по которому мы будем отбирать модель. Будем предпологать, что хорошая модель -- это модель, которая ошибается меньше всего.

Ошибка модели для i-ого наблюдения (человека) это разность между реальным значением и предсказанным.

$$income_i - (w_0 + w_1 education_i)$$

На графике это размер пунктирной линии. Видно, что ошибка может быть положительной (недопрогнозировали) или отрицательной (перепрогнозировали).

```{r echo=FALSE}
df <- df %>%
        mutate(predict = 10000 + 5000 * education)

ggplot(df, aes(x = education, y = income)) +
  geom_point()+
  geom_abline(intercept = 10000, slope = 5000, color = 'blue') +
  geom_segment(aes(x = education, y = predict, 
                   xend = education, yend = income),
               linetype = 'dashed')
```

Мы хотим взять и как-то объединить ошибки на всех наблюдениях в одну общую. Первая идея, которая приходит на ум, это найти среднюю ошибку. Эта величина и будет показывать ошибку модели на обучающей выборке. Мы будем называть её **функцией потерь (loss)**. 

$$loss = \frac{1}{n} \sum_{i=1}^{n} (income_i - w_0 - w_1 education_i)$$

Такая функция потерь не очень хороша. Представьте, что на 25 наблюдениях у вас ошибка 10, а на остальных 25 ошибка равна -10. В итоге функция потерь будет равна 0, что означает, что модель идеальна и не допускает ошибок. Положительные и отрицательные ошибки "схлопнули" друг друга. Давайте брать модуль или квадрат ошибки. Функция потерь в первом случае будет называться **MAE -- mean absolute error**, а во втором случае **MSE -- mean squared error**.

$$MAE = \frac{1}{n} \sum_{i=1}^{n} |income_i - w_0 - w_1 education_i|$$
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (income_i - w_0 - w_1 education_i)^2$$

### Обучение модели

Будем работать c MSE. Теперь мы определили, что такое ошибка модели. Будем считать, что та модель, у которой меньше MSE. Модели отличаются друг от друга **параметрами** $w_0$ и $w_1$. **Следовательно перед нами стоит задача минимизации функции потерь. Минимизировать эту функцию нужно по параметрам** $w_0$ и $w_1$. 

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (income_i - w_0 - w_1 education_i)^2 \rightarrow \min_{w_0,w_1}$$

На предыдущей посиделки мы научились минимизировать функции с помощью градиентного спуска! Давайте найдем градиент.

```{r}
set.seed(42)
n <- 50
df <- tibble(x = seq(7, 20, length.out = n) + rnorm(n, sd = 2),
             y = 1 + 2 * x + rnorm(n, sd = 0.3))

loss <- function(df, w0, w1){
  loss <- (df$y - w0 - w1 * df$x)^2
  return(sum(loss)/length(loss))
}

der_w0 <- function(df, w0, w1){
  x <- w0 + w1 * df$x - df$y 
  return(sum(x)/length(x))
}

der_w1 <- function(df, w0, w1){
  x <- (w0 + w1 * df$x - df$y) * df$x
  return(sum(x)/length(x))
}

GD <- function(df, w0, w1, al, nu) {
  x <- c(w0, w1)
  der <- c(der_w0(df, w0, w1), der_w1(df, w0, w1))
  nu <- al * nu + eta * der
  x <- x - nu
  return(x)
}
```

Давайте смотреть как мы делаем шаги градиентного спуска. Возьмем начальные веса $w_0 = -10$ и $w_1 = 10$.

```{r}
w0 <- 0.2
w1 <- -0.6

eta <-  0.001
alpha <-  0.4
nu <- 0
stage <- tibble(w0 = numeric(), w1 = numeric(), loss = numeric())

for(i in 1:10000){
  # p <- ggplot(df, aes(x, y)) +
  #         geom_point() +
  #         geom_abline(intercept = w0, slope = w1, color = 'blue') +
  #         ggtitle(paste('Значение потерь:',loss(df, w0, w1)))
  # print(p)
  # readline('Нажмите Enter, чтобы сделать шаг градиентного спуска:')
  stage <- add_row(stage, w0 = w0, w1 = w1, loss = loss(df, w0, w1))
  x <- c(w0, w1)
  der <- c(der_w0(df, w0, w1), der_w1(df, w0, w1))
  nu <- alpha * nu + eta * der
  x <- x - nu
  w0 <- x[1]
  w1 <- x[2]
}
```

Посмотрим на это дело в 3D.

```{r eval=FALSE}
x = seq(-2,2, length.out = 1000)
y = seq(-1,5, length.out = 1000)
z <- matrix(data = NA, nrow = 1000, ncol = 1000)
for(i in 1:nrow(z)){
  for(j in 1:ncol(z)){
    z[j,i] <- loss(df,x[i],y[j])
  }
}

pp <- plot_ly(z = z, x = x, y = y, type = "surface") %>%
        add_markers(x = stage$w0, y = stage$w1, z = stage$loss, size = 0.3)
pp
```

### Функция lm

```{r}
model <- lm(y ~ x, data = df)
summary(model)
```

<!-- ### Интерпретация полученных коэффициентов -->

<!-- ### Выбросы -->

<!-- ### Использование большего количества признаков -->


```{r}
n <- 100
df <- tibble(x1 = seq(-5, 5, length.out = n) + rnorm(n),
             x2 = seq(-10, 10, length.out = n) + rnorm(n),
             x3 = seq(12, 16, length.out = n) + rnorm(n),
             y = 5 + 2*x1 - 3*x2)
model <- lm(y ~ x1 + x2 + x3, data = df)
summary(model)
```


<!-- ### Категориальные переменные -->